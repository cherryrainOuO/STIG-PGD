"""
STIG-PGD Inference GUI
Gradio ê¸°ë°˜ ì¶”ë¡  ì¸í„°í˜ì´ìŠ¤
"""

import gradio as gr
import numpy as np
import os
import torch
from inference import inference_runner, InferenceConfig, load_model_for_gui, stop_inference_for_gui


def create_app():
    """Gradio ì•±ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    with gr.Blocks(title="STIG-PGD Inference") as app:
        
        # í—¤ë”
        gr.Markdown("""
        # ğŸ”¬ STIG-PGD Inference
        **Spectral Transform for Image Generation - Projected Gradient Descent**
        """)
        
        with gr.Row():
            # ì™¼ìª½ íŒ¨ë„: ì„¤ì •
            with gr.Column(scale=1):
                
                # ëª¨ë¸ ì„¤ì • ì„¹ì…˜
                gr.Markdown("### ğŸ“ ëª¨ë¸ ì„¤ì •")
                model_path = gr.Textbox(
                    label="ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ",
                    placeholder="ì˜ˆ: ./checkpoints/parameters_100_epoch.pt",
                    value=""
                )
                
                with gr.Row():
                    device_input = gr.Number(
                        label="GPU ë””ë°”ì´ìŠ¤",
                        value=0,
                        precision=0,
                        minimum=0
                    )
                    size_input = gr.Number(
                        label="ì´ë¯¸ì§€ í¬ê¸°",
                        value=256,
                        precision=0,
                        minimum=64
                    )
                
                # ë°ì´í„° ì„¤ì • ì„¹ì…˜
                gr.Markdown("### ğŸ“‚ ë°ì´í„° ì„¤ì •")
                data_path = gr.Textbox(
                    label="ì…ë ¥ ë°ì´í„° ê²½ë¡œ",
                    placeholder="ì˜ˆ: ./datasets/inference/",
                    value=""
                )
                save_path = gr.Textbox(
                    label="ì €ì¥ ê²½ë¡œ",
                    placeholder="ì˜ˆ: ./results/inference_output/",
                    value="./results/inference_output"
                )
                
                # ì‹¤í–‰ ë²„íŠ¼
                gr.Markdown("### â–¶ï¸ ì¶”ë¡  ì‹¤í–‰")
                with gr.Row():
                    start_btn = gr.Button("â–¶ï¸ ì¶”ë¡  ì‹œì‘", variant="primary")
                    stop_btn = gr.Button("â¹ï¸ ì¤‘ë‹¨", variant="stop")
                
                progress_text = gr.Textbox(
                    label="ì§„í–‰ ìƒí™©",
                    value="ëŒ€ê¸° ì¤‘...",
                    interactive=False
                )
                progress_bar = gr.Slider(
                    minimum=0,
                    maximum=100,
                    value=0,
                    label="ì§„í–‰ë¥  (%)",
                    interactive=False
                )
            
            # ì˜¤ë¥¸ìª½ íŒ¨ë„: ê²°ê³¼ ì¶œë ¥
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ–¼ï¸ ì¶”ë¡  ê²°ê³¼")
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("**ğŸ“¥ ì…ë ¥ ì´ë¯¸ì§€**")
                        input_image = gr.Image(
                            label="ì…ë ¥",
                            type="numpy",
                            interactive=False
                        )
                    with gr.Column():
                        gr.Markdown("**ğŸ“¤ ì¶œë ¥ ì´ë¯¸ì§€ (Denoised)**")
                        output_image = gr.Image(
                            label="ì¶œë ¥",
                            type="numpy",
                            interactive=False
                        )
        
        # í‘¸í„°
        gr.Markdown("""
        ---
        *STIG-PGD: AI ìƒì„± ì´ë¯¸ì§€ì˜ ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ê¸°ë°˜ í–¥ìƒ ëª¨ë¸*
        """)
        
        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        def on_start_inference(model_path_val, data_path_val, save_path_val, size_val, device_val):
            """ì¶”ë¡  ì‹œì‘ ë²„íŠ¼ í´ë¦­ í•¸ë“¤ëŸ¬ (ì œë„ˆë ˆì´í„°)"""
            # ëª¨ë¸ ê²½ë¡œ í™•ì¸
            if not model_path_val or not os.path.exists(model_path_val):
                yield None, None, "âŒ ìœ íš¨í•œ ëª¨ë¸ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", 0
                return
            
            if not data_path_val or not os.path.exists(data_path_val):
                yield None, None, "âŒ ìœ íš¨í•œ ë°ì´í„° ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", 0
                return
            
            # ëª¨ë¸ ë¡œë“œ
            yield None, None, "ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...", 0
            try:
                load_result = load_model_for_gui(model_path_val, int(device_val), int(size_val))
                if "âŒ" in load_result:
                    yield None, None, load_result, 0
                    return
            except Exception as e:
                yield None, None, f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}", 0
                return
            
            # ì„¤ì • ìƒì„±
            config = InferenceConfig(
                model_path=model_path_val,
                data_path=data_path_val,
                save_path=save_path_val,
                size=int(size_val),
                device=int(device_val)
            )
            
            # ì¶”ë¡  ì‹¤í–‰
            for input_img, output_img, status, current, total in inference_runner.run_inference(config):
                progress = int((current / max(total, 1)) * 100) if total > 0 else 0
                yield input_img, output_img, status, progress
        
        def on_stop_inference():
            """ì¶”ë¡  ì¤‘ë‹¨ ë²„íŠ¼ í´ë¦­ í•¸ë“¤ëŸ¬"""
            return stop_inference_for_gui()
        
        # ì´ë²¤íŠ¸ ì—°ê²°
        start_btn.click(
            fn=on_start_inference,
            inputs=[model_path, data_path, save_path, size_input, device_input],
            outputs=[input_image, output_image, progress_text, progress_bar]
        )
        
        stop_btn.click(
            fn=on_stop_inference,
            inputs=[],
            outputs=[progress_text]
        )
    
    return app


if __name__ == "__main__":
    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    print(f"ğŸ”§ CUDA Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"ğŸ”§ CUDA Device Count: {torch.cuda.device_count()}")
        print(f"ğŸ”§ CUDA Device Name: {torch.cuda.get_device_name(0)}")
    
    # ì•± ìƒì„± ë° ì‹¤í–‰
    app = create_app()
    app.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False,
        inbrowser=True
    )
