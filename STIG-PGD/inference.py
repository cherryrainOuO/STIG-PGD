import torch
from utils.util import OptionConfigurator, fix_randomseed
from utils.dataset import get_inference_dataset_from_option, InferenceDataset
from model.stig_pgd_model import STIG_PGD
from tqdm import tqdm
import os
from utils.visualizing import Visualizer
from utils.log import Logger
from utils.metric import InferenceModel
from torch.utils.data import DataLoader
from PIL import Image
from torchvision import transforms
import numpy as np
import matplotlib.pyplot as plt
from dataclasses import dataclass
from typing import Optional, Generator, Tuple
import threading


def inverse_norm(tensor):
    return (tensor + 1.) * 0.5


@dataclass
class InferenceConfig:
    """ì¶”ë¡  ì„¤ì •ì„ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    model_path: str
    data_path: str
    save_path: str
    size: int = 256
    batch_size: int = 1
    device: int = 0
    input_nc: int = 3
    output_nc: int = 3
    num_patch: int = 256
    sigma: int = 8
    norm: str = 'instance'
    nce_layers: str = '0,4,8,12,16'
    nce_idt: bool = True
    nce_T: float = 0.07
    lambda_GAN: float = 3.0
    lambda_NCE: float = 10.0
    lambda_PSD: float = 3.0
    lambda_LF: float = 3.0
    lambda_identity: float = 1.0
    lr: float = 0.00008
    beta1: float = 0.5
    beta2: float = 0.999


class InferenceRunner:
    """ì¶”ë¡  ì‹¤í–‰ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.model = None
        self.device = None
        self.is_running = False
        self.should_stop = False
        self._lock = threading.Lock()
    
    def load_model(self, config: InferenceConfig) -> str:
        """ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            fix_randomseed(42)
            
            is_gpu = torch.cuda.is_available()
            self.device = torch.device(config.device) if is_gpu else torch.device('cpu')
            
            # configë¥¼ opt í˜•íƒœë¡œ ë³€í™˜
            opt = self._config_to_opt(config)
            
            self.model = STIG_PGD(opt, self.device).to(self.device)
            self.model.load_checkpoint(config.model_path)
            self.model.eval()
            
            return f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {config.model_path}\në””ë°”ì´ìŠ¤: {self.device}"
        except Exception as e:
            return f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
    
    def _config_to_opt(self, config: InferenceConfig):
        """InferenceConfigë¥¼ argparse Namespace í˜•íƒœë¡œ ë³€í™˜"""
        class Opt:
            pass
        
        opt = Opt()
        opt.size = config.size
        opt.batch_size = config.batch_size
        opt.device = config.device
        opt.input_nc = config.input_nc
        opt.output_nc = config.output_nc
        opt.num_patch = config.num_patch
        opt.sigma = config.sigma
        opt.norm = config.norm
        opt.nce_layers = config.nce_layers
        opt.nce_idt = config.nce_idt
        opt.nce_T = config.nce_T
        opt.lambda_GAN = config.lambda_GAN
        opt.lambda_NCE = config.lambda_NCE
        opt.lambda_PSD = config.lambda_PSD
        opt.lambda_LF = config.lambda_LF
        opt.lambda_identity = config.lambda_identity
        opt.lr = config.lr
        opt.beta1 = config.beta1
        opt.beta2 = config.beta2
        opt.inference_data = config.data_path
        opt.inference_params = config.model_path
        
        return opt
    
    def stop_inference(self):
        """ì¶”ë¡ ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤."""
        with self._lock:
            self.should_stop = True
    
    def run_inference(
        self, 
        config: InferenceConfig,
        progress_callback=None
    ) -> Generator[Tuple[np.ndarray, np.ndarray, str, int, int], None, None]:
        """
        ì¶”ë¡ ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ yieldí•©ë‹ˆë‹¤.
        
        Yields:
            Tuple[input_image, output_image, save_path, current_idx, total_count]
        """
        with self._lock:
            if self.is_running:
                yield None, None, "ì´ë¯¸ ì¶”ë¡ ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.", 0, 0
                return
            self.is_running = True
            self.should_stop = False
        
        try:
            if self.model is None:
                yield None, None, "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", 0, 0
                return
            
            # ë°ì´í„°ì…‹ ì¤€ë¹„
            transform = transforms.Compose([
                transforms.Resize((config.size, config.size)),
                transforms.ToTensor(),
            ])
            
            # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            from glob import glob
            img_list = glob(os.path.join(config.data_path, '*.png'))
            img_list.extend(glob(os.path.join(config.data_path, '*.jpg')))
            img_list.extend(glob(os.path.join(config.data_path, '*.jpeg')))
            
            if not img_list:
                yield None, None, f"ë°ì´í„° ê²½ë¡œì— ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤: {config.data_path}", 0, 0
                return
            
            # ì €ì¥ ê²½ë¡œ ìƒì„±
            os.makedirs(config.save_path, exist_ok=True)
            denoised_path = os.path.join(config.save_path, 'denoised')
            denoised_mag_path = os.path.join(config.save_path, 'denoised_mag')
            os.makedirs(denoised_path, exist_ok=True)
            os.makedirs(denoised_mag_path, exist_ok=True)
            
            total_count = len(img_list)
            
            for idx, img_path in enumerate(img_list):
                # ì¤‘ë‹¨ ì²´í¬
                with self._lock:
                    if self.should_stop:
                        yield None, None, f"ì¶”ë¡  ì¤‘ë‹¨ë¨ ({idx}/{total_count})", idx, total_count
                        break
                
                # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
                image = Image.open(img_path).convert('RGB')
                input_tensor = transform(image).unsqueeze(0).to(self.device)
                
                # ì¶”ë¡  ì‹¤í–‰
                with torch.no_grad():
                    self.model.set_input(input_tensor, evaluation=True)
                    self.model.evaluation()
                
                # ê²°ê³¼ ì¶”ì¶œ
                input_img = self.model.input_image_normed.detach().squeeze(0)
                denoised_img = self.model.denoised_image_normed.detach().squeeze(0)
                denoised_mag = self.model.denoised_mag.detach().squeeze(0).mean(0)
                
                # numpyë¡œ ë³€í™˜
                input_np = np.transpose(input_img.cpu().numpy(), (1, 2, 0))
                denoised_np = np.transpose(denoised_img.cpu().numpy(), (1, 2, 0))
                denoised_mag_np = np.clip(denoised_mag.cpu().numpy(), 0.0, 1.0)
                
                # ì´ë¯¸ì§€ ì €ì¥
                save_name = f'{idx:06d}.png'
                plt.imsave(os.path.join(denoised_path, save_name), denoised_np)
                plt.imsave(os.path.join(denoised_mag_path, save_name), denoised_mag_np, cmap='jet')
                
                yield input_np, denoised_np, f"ì²˜ë¦¬ ì¤‘: {idx+1}/{total_count}", idx+1, total_count
            
            yield None, None, f"âœ… ì¶”ë¡  ì™„ë£Œ! ì €ì¥ ê²½ë¡œ: {config.save_path}", total_count, total_count
            
        except Exception as e:
            yield None, None, f"âŒ ì¶”ë¡  ì˜¤ë¥˜: {str(e)}", 0, 0
        finally:
            with self._lock:
                self.is_running = False
                self.should_stop = False


# ì „ì—­ ì¶”ë¡  ëŸ¬ë„ˆ ì¸ìŠ¤í„´ìŠ¤
inference_runner = InferenceRunner()


def load_model_for_gui(model_path: str, device: int = 0, size: int = 256) -> str:
    """GUIì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    config = InferenceConfig(
        model_path=model_path,
        data_path="",
        save_path="",
        device=device,
        size=size
    )
    return inference_runner.load_model(config)


def run_inference_for_gui(
    data_path: str,
    save_path: str,
    size: int = 256,
    device: int = 0
):
    """GUIì—ì„œ ì¶”ë¡ ì„ ì‹¤í–‰í•˜ëŠ” ì œë„ˆë ˆì´í„° í•¨ìˆ˜"""
    config = InferenceConfig(
        model_path="",  # ì´ë¯¸ ë¡œë“œë¨
        data_path=data_path,
        save_path=save_path,
        size=size,
        device=device
    )
    
    for result in inference_runner.run_inference(config):
        yield result


def stop_inference_for_gui():
    """GUIì—ì„œ ì¶”ë¡ ì„ ì¤‘ë‹¨í•˜ëŠ” í•¨ìˆ˜"""
    inference_runner.stop_inference()
    return "ğŸ›‘ ì¶”ë¡  ì¤‘ë‹¨ ìš”ì²­ë¨"


if __name__ == '__main__':
    # CLI ëª¨ë“œë¡œ ì‹¤í–‰
    fix_randomseed(42)
    
    opt = OptionConfigurator().parse_options()
    loader = get_inference_dataset_from_option(opt)

    is_gpu = torch.cuda.is_available()
    device = torch.device(opt.device) if is_gpu else torch.device('cpu')

    model = STIG_PGD(opt, device).to(device)
    model.load_checkpoint(opt.inference_params)

    save_path = os.path.join('./results', opt.dst)
    os.makedirs(save_path, exist_ok=True)
    os.makedirs(os.path.join(save_path, 'inference'), exist_ok=True)
    inferencer = InferenceModel(opt, os.path.join(save_path, 'inference'))

    for n, sample in enumerate(tqdm(loader, desc="{:17s}".format('Inference State'), mininterval=0.0001)):
        model.set_input(sample, evaluation=True)
        model.evaluation()
        inferencer.step(model, sample, n)
        

